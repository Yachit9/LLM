In this repository we will be creating an LLM from scratch,this is done with the help of Sebastian Raschka notebooks on github.
1. Text preprocessing:
   Here we did the text prprocessing firstly we created a simple tokenizer which did tokenize the text and gave it the token id's accordingly.Then we used the tiktoken library's GPT style byte pair encoding where it tokenized the text and then we did data loading with the help of sliding window approach where we have taken a size of the window for inputs and that for targets where targets are moving i+1 from the inputs and at the end we create a dataloader with the help of it.
2. LLM Architecture:
   Now what we learned here is that after text encoding or byte pair encoding it goes through a transformer block where muti head self attention blocks with QKV mechanism are there and then after that feed forward neural networks and linear neural networks are also present. Basically it is the same transformer architecture presented in the Research paper attention is all you need and then after that these embeddings are used to predict the next word in the sentence using the one having highest probability. This second ipynb file tells us how basically an LLM architecture is made and how it is used to predict another word. Next we will pretrain the model.
